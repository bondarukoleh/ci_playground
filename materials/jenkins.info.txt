Jenkins is an open source continuous integration CI and continuous delivery tool written in Java.
It's an automation server used to build and deliver SW projects.
Was forked from another a-like server Hudson, after dispute or fight with Oracle.
It's cool to pack app in docker since it is isolated environment, and you can be more sure that your app works.
Major feature in Jenkins - plugins.

Jenkins is open source and well known but there are alternatives:
  - Self-hosted: Drone CI, TeamCity (by JetBrains)
  - Hosted (SaaS): Wercker, CircleCI, CodeShip, SemaphoreCI, Amazone AWS CI/CI tools.

Continuous Integration - is the practice of merging all developer working copies to a shared mainline several
times a day. Continuous Delivery - is the approach in which teams produce SW in short cycles, ensures that.
SW can be released in any moment. In practice CI/CD says that we will verify and publish SW by triggering
automated builds and test.

All developers push their changes to VCS where code tested at least once a day.
CI/CD with SDLC (SW development life cycle)
New Feature (by dev) ->
 Push (be dev) ->
  Build (by Jenkins) ->
   Test (Unit, Integration, Regression, Acceptance, etc) (by Jenkins) ->
    Release (Package SW, to tar, zip, jar, docker) (by Jenkins) ->
     :optional Distribute(to private storage/docker registry) ->
      Deploy to production(by Jenkins).

To install Jenkins with docker:
mkdir -p /var/jenkins_home
chown -R 1000:1000 /var/jenkins_home/
docker run -p 8080:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home -d --name jenkins jenkins/jenkins:lts

We can make a jenkins with docker client, so jenkins container can run docker commands.

It's easy to setup Jenkins via UI. But there are several disadvantages of this approach:
- No audit trail, you'll never know who make a change, that break something.
- No history of changes, you don't know what was changed, that break something.
- No ability to easy backup or restore Jenkins setup.
- Usually developers don't have rights to change Jenkins setup, admins have, it takes time.

Many of those troubles will be solved if we will store Jenkins setup in Jenkins DSL, in VCS.

Jenkins Jobs
Job DSL - Jenkins plugin that allows to define jobs in programmatic way. It use Groovy based language
 Groovy - is like scripting language for Java platform, runs in JVM.
So we create a job in Jenkins that will be creating jobs with DSL.
We can download DSL scripts from repositories from Process Job DSLs Build block - but first
  we need to approve using them. In Manage -> In-process Script Approval -> Approve

If node is executable on jenkins, since all ls or copy or npm commands is looked in $PATH
we need to add npm to $PATH.
Easy way to find where npm is installed.
jenkins@$>npm start
bash: npm: command not found
jenkins@$>find ~ -name 'npm'
jenkins@$>export PATH=$PATH:/var/jenkins_home/tools/jenkins.plugins.nodejs.tools.NodeJSInstallation/nodejs/bin

Also don't forget that you need to enable the "Provide Node & npm bin/ folder to PATH" in job configuration

As we can see from registryCredentials('dockerhub') in DSL script - there are id from credentials to
  dockerhub, to push created image. You can add it in Credentials (left menu, not in manage) ->
    Add -> and set ID - same as in script 'dockerhub'

Jenkins Pipelines
Allow you to write Jenkins build steps in code.
Build steps allow you to write: build (compile), test, deploy in code, mean you can put setup
for all of these steps - in VCS.
It's a way to automate SDLC.
Pipelines is a type of Jobs, we can do a huge "free style" Job and do same as in pipeline,
difference is in implementation in Jenkins.
Cool feature - is "Organization folder" - pipeline job that have separately diff repositories,
and you can logically divide build steps.

Pipeline can be created via UI or Job DSL (Jenkins DSL or Groovy script)
Jenkins DSL will be interpreted by Groovy under the hood anyway.

Pay attention, when you did a job groovy script - you created a general project
that creates a job from provided job script. When you do a pipeline - it doesn't
create some extra entities like another pipeline or job - it is a pipeline that
would do the steps described in script. But still you've created pipeline manually,
which I think could be done in code.

Benefits of keeping jenkins pipeline groovy script within the app code is huge.
You don't need the taskFile.sh or start-tests.sh in your library, you don't need
to change anything, in your created pipeline on Jenkins side - changes in app code
can be supported in jenkins file that lies here with the code, incredible.

Docker pipeline let's you not only build but run any container within pipeline.
For example you want to pack app inside the container, but only production code,
and still want to test and develop it in isolated environment mean use containers.
So we can build a large container with tests and all stuff, run it, test it,
and then build a container only with tested prod code and push it.

Also we can add any container with some stuff, during Pipeline, use it, and remove
after tests are done e.g. container with test mocked DB, run tests on it, and
remove it after testing stage. Also it works with multiple builds at the same time,
e.g. you are building a few versions of app - and you'll get a clean new DB container
for each of those which is pretty cool.

It's an alternative way to run for instance node commands - you remember that you need
to setup "Build Environment" for Jenkins and put a NodeJS installation path in it?
It's much better way to have ability to get NodeJS environment dynamically with
different versions of it.

**Note about inside, run, withRun:
Image.run([args, command])
Uses docker run to run the image, and returns a Container which you could stop later.
Additional args may be added, such as '-p 8080:8080 --memory-swap=-1'. Optional command
is equivalent to Docker command specified after the image. Records a run fingerprint in
the build.

Image.withRun[(args[, command])] {…}
Like run but stops the container as soon as its body exits, so you do not need a try-finally
block.

Image.inside[(args)] {…}
Like withRun this starts a container for the duration of the body, but all external commands
(sh) launched by the body run inside the container rather than on the host. These commands
run in the same working directory (normally a Jenkins agent workspace), which means that
the Docker server must be on localhost.

Emailing
We can send an email with build result. But be aware that google will fight with this.
The earlier dev gets the fail the better, solution is more or less fresh in his head,
and he can fix it more quickly.
Build should run on each commit in master branch.
VSC can be pulled by Jenkins, means Jenkins asking e.g. each 5 minutes - is there any
change? This functionality is in Jenkins.

Or Jenkins could be pushed from VSC - there was some change - make a build. There are
a few plugins github, bitbucket to configure this functionality.

Slack integration.
So we can see messages like build failure, someone picked up this problem, someone
pushed new changes, Jenkins start a job, Jenkins job was successful, new build is
packed and pushed to registry, new build was deployed on est env, Jenkins says
that everything green again and devs could working further, etc. - this called ChatOps.
So everyone has up-to-date information about the builds, everyone has same history
in chat, and everyone happy.

We can add integration via webhooks, and this is pretty powerful tool - you can setup to send pictures,
maybe some tables, many kids of data to slack via POST message.
Or we can use Jenkins CI plugin in Slack - and some regular slack plugin in Jenkins.

VCS integration.
Jenkins can autoscan all branches and repositories in Github organization or teams/projects in Bitbucket
with GitHub/Bitbucket Branch Source plugin. It gives ability to automatically create projects from those
sources and don't do anything, Jenkins will do all job by himself - cool.

So we can create a "Github organization" project in Jenkins, and setup a connection to some organization
in github, set a patterns to find needed branches and files - and don't bother creating a pipelines manually.
Jenkins will scan repositories and branches and will create a pipeline by himself.
It's a coll feature when we have a lot of projects, and a lot of repositories with a few branches - and
can test separately microservice from My_Project -> Firewall_repo -> new_feature_branch pipeline project,
When developer creates a new branch -> Jenkins scans it and creates a new job with this branch - and dev can
change something - and re-run this job - which will make a new build of this ob with new artifact, and you
can just sit and relax. Looks great.

We can use JFrog artifactory plugin to integrate with JFrog - artifactory store, place where we can keep
job artifacts.


Custom API integration
Jenkins has http request plugin - that allows you to make http requests. You can install it and in jenkins
file, depend on stage and results - make a requests to your super custom service.
To understand format of requests from Jenkins we can use helper. Go to your pipeline -> configure ->
pipeline-syntax -> and there are a lot of stuff that you can use node commands, read files, and http requests
 if appropriate the plugin installed.

Sonarqube integration.
Sonarqube continuously inspects you code on:
- Bugs (code issues)
- Vulnerabilities (security issues)
- Code smells (maintainability issues)
- Technical dept (estimated time required to fix)
- Code coverage (test coverage of code)

In Jenkins usually it's a build step - code scanning with plugin. Sonarqube sends results to sonarqube server,
it needs to be installed, server uses PostgreSQL DB to maintain it's state.
We can run the sonarqube server and DB in docker containers on master node near the Jenkins container e.g.
We will use docker-compose, don't forget that it needs to be installed separately from docker.

Jenkins Slaves
In production it's usual to have one small master node, where Jenkins UI is running, and one or more
worker nodes, Jenkins slaves - to expand build capacity. Typically one worker - has one or more build
executors (building slots). If Jenkins node has 2 executors - it can build only 2 builds in parallel.
Static (manual) / dynamic scaling
A way to setup number of workers that should be increased or available in working hours, or maybe added
adhoc when all are busy, and reducing or not-available workers in non-worker hours.
e.g. Amazon EC2 plugin lets you scale the workers automatically and kill them when they aren't busy.
Docker plugin - using a docker host can run a jenkins-docker slave to do some work, and then kill it.
Amazon ECS plugin - same as Docker but uses docker orchestrator EC2 Container engine (amazon docker cluster
that you need to setup) to rule docker-slaves.
Digital ocean plugin - dynamically provisions droplets to be used as jenkins slaves.
Builds can be executed on specific nodes. Nodes can be labeled, and setup from UI or in jenkinsfile.
Benefits (cloud mostly):
- scalability - pay only for working hours
- slaves can be easily replaced (if crashed) (with properly installed)
- slave is not affecting the master node - if it took 100% CPU - you can still manage master.

Do not install tools manually use everything in dockers, or in plugins. We need to have stupid slave
easy to remove and switch, because all environment should be provided in code, from master node.

Two ways to connection between slave and master:
- from master to slave via ssh.
- from slave to master via JNLP (good if slaves behind firewall, these slaves they should
only have java installed and you can connect them with one click without ssh headache, they
will be connected via JNLP protocol)

When you want to setup ssh connection, you have to create .shh keys on slave machine (via script)
add a .pub master key to slave, draft script you can find in materials zip.

Blue Ocean - is a new UI plugin for Jenkins. Not fully, but it will replace with time an old UI.

SSH-agent
When you need to reach private repo from jenkins - one way is manually generate pub/private keys
on machine, and pass pub key to github account, but private key you need to store in jenkins.
We can install SSH Agent plugin, and use "sshagent(credentials: ['my_git_cred_id']){}" in jenkins
file to get private repo, but still private key will be on Jenkins side. The cool thing about it -
that you don't need to manually add a credentials for repo - you can create them in jenkins and
use in jenkins file. Also - even if you manually added a credentials in you pipeline - you cannot
use them without sshagent from jenkinsfile, from the code, that's the thing. And it should run
on slave jenkins also - so the idea not to manually setup slave machine - but reach repo with
credentials that stored on master jenkins - that the things as far as i get.

To easy add pub ssh key from github to the jenkins machine
ssh-keyscan github.com >> /var/jenkins_home/.ssh/known_hosts

Security
Try to store your Jenkins up-to-date (lts) behind VPN or Firewall (with github/bitbucket in whitelist).
Authentication and Authorization
Authentication - verifying identity of a user or a process. "Are you who you've claimed you are?"
Authorization - verifying that some user or process has access to resource it asked. Do you have permissions
to do what you want to do?

Authorization:
- anyone can do anything
- anyone who logged in can do anything
- Matrix based authorization (table with user/access rows)

In Jenkins it is a RoleStrategy plugin - that implements extended Matrix based authorization.
If something goes wrong, and you've locked yourself:
$>docker stop jenkins
$>vim /var/jenkins_home/config.xml
<useSecurity>false</useSecurity> - remove or edit
<authorizationStrategy class="hudson.security.ProjectMatrixAuthorizationStrategy">
<permissions>hudson.model.Hudson.Administer:YOUR_USER</permissions>
</authorizationStrategy>
$>docker start jenkins

For security we go to Manage jenkins -> Configure Global Security
We can see that Jenkins expose two ports 8080 and 50000 - for slaves. (TCP port and JNLP protocol)
So these ports should be blocked if you put Jenkins behind firewall.
In Authorization part - you can choose the strategy, for example matrix based.
You can check users in Main Jenkins left panel -> People.

Jenkins users
Common practice is to store user in some database, not in jenkins.
Companies often using directory service like Active Directory or LDAP
Directory service - is a hierarchy access control server, that provide access to some resource
depend on it's class (basically something like it)
So it's a common practice to connect Jenkins to such service in Security Realm part of
Configure Global Security.
AD service can be your own or hosted solution.
One of the nice identity provider hosted solutions - onelogin.com
We can link Jenkins to onelogin using SAML (another option near LDAP, to use - install plugin)
SAML - Security Assertion Markup Language, xml based open-standard data format for exchanging
authentication and authorization data between parties.

Allure
Simple manually adding allure.
In Jenkins "Plugin manager" install Allure plugin. Restart Jenkins.
In Jenkins "Global Tool Configuration" add Allure Commandline with versions.
In Job configuration add Post build action "Allure Report"

Check the https://docs.qameta.io/allure/#_jenkins

Magic in groovy files job(), shell(), etc. - it's a Job DSL plugin
https://plugins.jenkins.io/job-dsl/
https://jenkinsci.github.io/job-dsl-plugin/#
